{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmNjIyFSlyZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGlbNV6Gl8d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this part will be used for importing files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLpS1DeWmVI9",
        "colab_type": "text"
      },
      "source": [
        "# GANS have 2 basic parts, generator and discriminator. The Generator we will use is UNET Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekro5sFcmlZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a Generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8k_fzHEllWY",
        "colab_type": "text"
      },
      "source": [
        "The architecture of the generator is a modified UNET<br>\n",
        "Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)<br>\n",
        "The encoder downsamples to a basic form , from where upsampling is done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywP8OAdWkrUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code for downsampling\n",
        "def downsample(filters,size,apply_batchnormal=True):\n",
        "  initializer=tf.random_normal_initializer(0,0.02)\n",
        "\n",
        "  result=tf.keras.Sequential()\n",
        "  result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnormal:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "  \n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "  \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGs3kdFP_VNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code for upsampling\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jA-lOueB4uf",
        "colab_type": "text"
      },
      "source": [
        "The encoder-decoder architecture consists of:<br>\n",
        "**encoder**:<br>\n",
        "C64-C128-C256-C512-C512-C512-C512-C512<br>\n",
        "**decode**r:<br>\n",
        "CD512-CD512-CD512-C512-C256-C128-C64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlQx9a7NKuuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_CHANNELS=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnWl4EIkLX4p",
        "colab_type": "text"
      },
      "source": [
        "After the last layer in the decoder, a convolution is ap-\n",
        "plied to map to the number of output channels (3 in general,\n",
        "except in colorization, where it is 2), followed by a Tanh\n",
        "function. As an exception to the above notation, Batch-\n",
        "Norm is not applied to the first C64 layer in the encoder.\n",
        "All ReLUs in the encoder are leaky, with slope 0.2, while\n",
        "ReLUs in the decoder are not leaky.\n",
        "The U-Net architecture is identical except with skip con-\n",
        "nections between each layer i in the encoder and layer n − i\n",
        "in the decoder, where n is the total number of layers. The\n",
        "skip connections concatenate activations from layer i to\n",
        "layer n − i. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPok0sby_fho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnormal=False), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4XSToS7MTyd",
        "colab_type": "text"
      },
      "source": [
        "Our final objective is\n",
        "G ∗ = arg min max L cGAN (G, D) + λL L1 (G)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkSuXQ6EMcO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  LAMBDA=100\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb_QMRaXOfMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ghmr8oTYBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#discriminator loss\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfn3rLKqTlpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Optimisers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SWw0tvoUedO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator=Generator()\n",
        "discriminator=Discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEemUeKEVhuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2OO41YFUix2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9gYTYhcVMMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}